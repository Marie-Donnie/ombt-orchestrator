{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The path to the env dir of the experimental campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = \"../../results/threading_eventlet/test_case_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting some ombt code (this could be removed if used as a library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Stats(object):\n",
    "    \"\"\"Manage a single statistic\"\"\"\n",
    "    def __init__(self, min=None, max=None, total=0, count=0,\n",
    "                 sum_of_squares=0, distribution=None):\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        self.total = total\n",
    "        self.count = count\n",
    "        self.sum_of_squares = sum_of_squares\n",
    "        # distribution of values grouped by powers of 10\n",
    "        self.distribution = distribution or dict()\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, values):\n",
    "        if 'distribution' in values:\n",
    "            # hack alert!\n",
    "            # when a Stats is passed via an RPC call it appears as if the\n",
    "            # distribution map's keys are converted from int to str.\n",
    "            # Fix that by re-indexing the distribution map:\n",
    "            new_dict = dict()\n",
    "            old_dict = values['distribution']\n",
    "            for k in old_dict.keys():\n",
    "                new_dict[int(k)] = old_dict[k];\n",
    "            values['distribution'] = new_dict\n",
    "        return Stats(**values)\n",
    "\n",
    "    def to_dict(self):\n",
    "        new_dict = dict()\n",
    "        for a in [\"min\", \"max\", \"total\", \"count\", \"sum_of_squares\"]:\n",
    "            new_dict[a] = getattr(self, a)\n",
    "        new_dict[\"distribution\"] = self.distribution.copy()\n",
    "        return new_dict\n",
    "\n",
    "    def update(self, value):\n",
    "        self.total += value\n",
    "        self.count += 1\n",
    "        self.sum_of_squares += value**2\n",
    "        self.min = min(self.min, value) if self.min else value\n",
    "        self.max = max(self.max, value) if self.max else value\n",
    "        log = int(math.log10(value)) if value >= 1.0 else 0\n",
    "        base = 10**log\n",
    "        index = int(value/base)  # 0..9\n",
    "        if log not in self.distribution:\n",
    "            self.distribution[log] = [0 for i in range(10)]\n",
    "        self.distribution[log][index] += 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "\n",
    "    def average(self):\n",
    "        return (self.total / float(self.count)) if self.count else 0\n",
    "\n",
    "    def std_deviation(self):\n",
    "        return math.sqrt((self.sum_of_squares / float(self.count))\n",
    "                         - (self.average() ** 2)) if self.count else -1\n",
    "\n",
    "    def merge(self, stats):\n",
    "        if stats.min is not None and self.min is not None:\n",
    "            self.min = min(self.min, stats.min)\n",
    "        else:\n",
    "            self.min = self.min or stats.min\n",
    "        if stats.max is not None and self.max is not None:\n",
    "            self.max = max(self.max, stats.max)\n",
    "        else:\n",
    "            self.max = self.max or stats.max\n",
    "\n",
    "        self.total += stats.total\n",
    "        self.count += stats.count\n",
    "        self.sum_of_squares += stats.sum_of_squares\n",
    "        for k in stats.distribution.keys():\n",
    "            if k in self.distribution:\n",
    "                self.distribution[k] = [z for z in map(lambda a, b: a + b,\n",
    "                                                       stats.distribution[k],\n",
    "                                                       self.distribution[k])]\n",
    "            else:\n",
    "                self.distribution[k] = stats.distribution[k]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"min=%i, max=%i, avg=%f, std-dev=%f\" % (self.min, self.max,\n",
    "                                                       self.average(),\n",
    "                                                       self.std_deviation())\n",
    "\n",
    "    def print_distribution(self):\n",
    "        keys = list(self.distribution.keys())\n",
    "        keys.sort()\n",
    "        for order in keys:\n",
    "            row = self.distribution[order]\n",
    "            # order=0, index=0 is special case as it is < 1.0, for all orders >\n",
    "            # 0, index 0 is ignored since everthing < 10^order is accounted for\n",
    "            # in index 9 of the (order - 1) row\n",
    "            index = 0 if order == 0 else 1\n",
    "            while index < len(row):\n",
    "                print(\"[%d..<%d):  %d\" %\n",
    "                      ((10 ** int(order)) * index,\n",
    "                       (10 ** int(order)) * (index + 1),\n",
    "                       row[index]))\n",
    "                index += 1\n",
    "\n",
    "class TestResults(object):\n",
    "    \"\"\"Client results of a test run.\n",
    "    \"\"\"\n",
    "    def __init__(self, start_time=None, stop_time=None, latency=None,\n",
    "                 msgs_ok=0, msgs_fail=0, errors=None):\n",
    "        super(TestResults, self).__init__()\n",
    "        self.start_time = start_time\n",
    "        self.stop_time = stop_time\n",
    "        self.latency = latency or Stats()\n",
    "        self.msgs_ok = msgs_ok  # count of successful msg transfers\n",
    "        self.msgs_fail = msgs_fail  # count of failed msg transfers\n",
    "        self.errors = errors or dict()  # error msgs and counts\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, values):\n",
    "        if 'latency' in values:\n",
    "            values['latency'] = Stats.from_dict(values['latency'])\n",
    "        if 'errors' in values:\n",
    "            values['errors'] = values['errors'].copy()\n",
    "        return TestResults(**values)\n",
    "\n",
    "    def to_dict(self):\n",
    "        new_dict = dict()\n",
    "        for a in ['start_time', 'stop_time', 'msgs_ok', 'msgs_fail']:\n",
    "            new_dict[a] = getattr(self, a)\n",
    "        new_dict['latency'] = self.latency.to_dict()\n",
    "        new_dict['errors'] = self.errors.copy()\n",
    "        return new_dict\n",
    "\n",
    "    def error(self, reason):\n",
    "        key = str(reason)\n",
    "        self.errors[key] = self.errors.get(key, 0) + 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "\n",
    "    def merge(self, results):\n",
    "        self.start_time = (min(self.start_time, results.start_time)\n",
    "                           if self.start_time and results.start_time\n",
    "                           else (self.start_time or results.start_time))\n",
    "        self.stop_time = (max(self.stop_time, results.stop_time)\n",
    "                              if self.stop_time and results.stop_time\n",
    "                          else (self.stop_time or results.stop_time))\n",
    "        self.msgs_ok += results.msgs_ok\n",
    "        self.msgs_fail += results.msgs_fail\n",
    "        self.latency.merge(results.latency)\n",
    "        for err in results.errors:\n",
    "            self.errors[err] = self.errors.get(err, 0) + results.errors[err]\n",
    "\n",
    "    def print_results(self):\n",
    "        if self.msgs_fail:\n",
    "            print(\"Error: %d message transfers failed\"\n",
    "                  % self.msgs_fail)\n",
    "        if self.errors:\n",
    "            print(\"Error: errors detected:\")\n",
    "            for err in self.errors:\n",
    "                print(\"  '%s' (occurred %d times)\" % (err, self.errors[err]))\n",
    "\n",
    "        total = self.msgs_ok + self.msgs_fail\n",
    "        print(\"Total Messages: %d\" % total)\n",
    "\n",
    "        delta_time = self.stop_time - self.start_time\n",
    "        print(\"Test Interval: %f - %f (%f secs)\" % (self.start_time,\n",
    "                                                    self.stop_time,\n",
    "                                                    delta_time))\n",
    "\n",
    "        if delta_time > 0.0:\n",
    "            print(\"Aggregate throughput: %f msgs/sec\" % (float(total)/delta_time))\n",
    "\n",
    "        latency = self.latency\n",
    "        if latency.count:\n",
    "            print(\"Latency %d samples (msecs): Average %f StdDev %f\"\n",
    "                  \" Min %f Max %f\"\n",
    "                  % (latency.count,\n",
    "                     latency.average(), latency.std_deviation(),\n",
    "                     latency.min, latency.max))\n",
    "            print(\"Latency Distribution: \")\n",
    "            latency.print_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stats(param):\n",
    "    \"\"\"Loads the stats for the controller output file.\"\"\"\n",
    "    \n",
    "    controller_docker = os.path.join(RESULT_PATH, param[\"backup_dir\"], \"*controller*.log\")\n",
    "    # beware of the files _docker.log that would also match\n",
    "    # take [0] to get rid of them for now\n",
    "    files = glob.glob(controller_docker)\n",
    "    controller_log = files[0]\n",
    "    a = []\n",
    "    with open(controller_log) as f:\n",
    "        a = f.readlines()\n",
    "        return json.loads(a[0]), json.loads(a[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "with open(os.path.join(RESULT_PATH, \"./params.json\")) as f:\n",
    "    params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agg_results(results):\n",
    "    agg = TestResults()\n",
    "    for result in results:\n",
    "        result[\"latency\"] = Stats(**result[\"latency\"])\n",
    "        agg.merge(TestResults(**result))\n",
    "        \n",
    "    duration = agg.stop_time - agg.start_time\n",
    "    total = agg.msgs_ok + agg.msgs_fail\n",
    "    rate = float(total)/duration\n",
    "    result = agg.to_dict()\n",
    "    result[\"rate\"] = rate\n",
    "    return result\n",
    "\n",
    "    \n",
    "for param in params:\n",
    "    clients, servers = load_stats(param)\n",
    "    # what has been seen by ombt\n",
    "    param[\"_ombt_clients\"] = len(clients.values())\n",
    "    param[\"_ombt_servers\"] = len(servers.values())\n",
    "    #param[\"_raw_servers_test_result\"] = servers\n",
    "    #param[\"_raw_clients_test_result\"] = clients\n",
    "    param[\"_agg_servers\"] = build_agg_results(servers.values())\n",
    "    param[\"_agg_clients\"] = build_agg_results(clients.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"params_calculated.json\", \"w\") as f:\n",
    "    json.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(mydict, myparams, in_key, out_key=None):\n",
    "    out_key = out_key or in_key\n",
    "    mydict.update({out_key: [p[in_key] for p in myparams]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction = {}\n",
    "augment(extraction, params, \"_ombt_clients\", \"clients\")\n",
    "augment(extraction, params, \"_ombt_servers\", \"servers\")\n",
    "augment(extraction, params, \"executor\")\n",
    "augment(extraction, params, \"call_type\")\n",
    "extraction.update({\n",
    "    \"server_rate\": [p[\"_agg_servers\"][\"rate\"] for p in params]\n",
    "})\n",
    "extraction.update({\n",
    "    \"client_rate\": [p[\"_agg_clients\"][\"rate\"] for p in params]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call_type</th>\n",
       "      <th>client_rate</th>\n",
       "      <th>clients</th>\n",
       "      <th>executor</th>\n",
       "      <th>server_rate</th>\n",
       "      <th>servers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rpc-call</td>\n",
       "      <td>371.334883</td>\n",
       "      <td>1</td>\n",
       "      <td>threading</td>\n",
       "      <td>371.778178</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rpc-call</td>\n",
       "      <td>348.927736</td>\n",
       "      <td>1</td>\n",
       "      <td>eventlet</td>\n",
       "      <td>349.326537</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rpc-cast</td>\n",
       "      <td>1344.599280</td>\n",
       "      <td>1</td>\n",
       "      <td>eventlet</td>\n",
       "      <td>1347.332930</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rpc-cast</td>\n",
       "      <td>1156.207023</td>\n",
       "      <td>1</td>\n",
       "      <td>threading</td>\n",
       "      <td>1158.226457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rpc-call</td>\n",
       "      <td>713.775620</td>\n",
       "      <td>2</td>\n",
       "      <td>threading</td>\n",
       "      <td>714.620277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rpc-cast</td>\n",
       "      <td>2489.178289</td>\n",
       "      <td>2</td>\n",
       "      <td>threading</td>\n",
       "      <td>2493.470423</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rpc-call</td>\n",
       "      <td>754.701880</td>\n",
       "      <td>2</td>\n",
       "      <td>eventlet</td>\n",
       "      <td>755.469575</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rpc-call</td>\n",
       "      <td>675.494213</td>\n",
       "      <td>2</td>\n",
       "      <td>eventlet</td>\n",
       "      <td>676.310842</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rpc-cast</td>\n",
       "      <td>2337.756858</td>\n",
       "      <td>2</td>\n",
       "      <td>threading</td>\n",
       "      <td>1744.275477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rpc-cast</td>\n",
       "      <td>2550.988087</td>\n",
       "      <td>2</td>\n",
       "      <td>eventlet</td>\n",
       "      <td>2554.089459</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rpc-cast</td>\n",
       "      <td>2236.801084</td>\n",
       "      <td>2</td>\n",
       "      <td>eventlet</td>\n",
       "      <td>1762.513665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rpc-call</td>\n",
       "      <td>735.813245</td>\n",
       "      <td>2</td>\n",
       "      <td>threading</td>\n",
       "      <td>736.676479</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rpc-call</td>\n",
       "      <td>1384.631637</td>\n",
       "      <td>4</td>\n",
       "      <td>eventlet</td>\n",
       "      <td>1386.001425</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rpc-cast</td>\n",
       "      <td>5020.741978</td>\n",
       "      <td>4</td>\n",
       "      <td>eventlet</td>\n",
       "      <td>3506.265197</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rpc-cast</td>\n",
       "      <td>4980.823320</td>\n",
       "      <td>4</td>\n",
       "      <td>threading</td>\n",
       "      <td>3464.001544</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rpc-call</td>\n",
       "      <td>1395.826369</td>\n",
       "      <td>4</td>\n",
       "      <td>threading</td>\n",
       "      <td>1397.605533</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   call_type  client_rate  clients   executor  server_rate  servers\n",
       "0   rpc-call   371.334883        1  threading   371.778178        1\n",
       "1   rpc-call   348.927736        1   eventlet   349.326537        1\n",
       "2   rpc-cast  1344.599280        1   eventlet  1347.332930        1\n",
       "3   rpc-cast  1156.207023        1  threading  1158.226457        1\n",
       "4   rpc-call   713.775620        2  threading   714.620277        1\n",
       "5   rpc-cast  2489.178289        2  threading  2493.470423        2\n",
       "6   rpc-call   754.701880        2   eventlet   755.469575        2\n",
       "7   rpc-call   675.494213        2   eventlet   676.310842        1\n",
       "8   rpc-cast  2337.756858        2  threading  1744.275477        1\n",
       "9   rpc-cast  2550.988087        2   eventlet  2554.089459        2\n",
       "10  rpc-cast  2236.801084        2   eventlet  1762.513665        1\n",
       "11  rpc-call   735.813245        2  threading   736.676479        2\n",
       "12  rpc-call  1384.631637        4   eventlet  1386.001425        2\n",
       "13  rpc-cast  5020.741978        4   eventlet  3506.265197        2\n",
       "14  rpc-cast  4980.823320        4  threading  3464.001544        2\n",
       "15  rpc-call  1395.826369        4  threading  1397.605533        2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.DataFrame(extraction)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recovering metrics from influxdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "from influxdb import InfluxDBClient\n",
    "\n",
    "client = docker.from_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the interesting metrics from influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T09:48:10Z', 'max': 0}]})\n",
      "1 1 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T09:50:15Z', 'max': 2002}]})\n",
      "1 1 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T09:52:00Z', 'max': 1002}]})\n",
      "1 1 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T09:53:45Z', 'max': 1002}]})\n",
      "2 1 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T09:57:05Z', 'max': 4004}]})\n",
      "2 2 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T09:59:00Z', 'max': 2004}]})\n",
      "2 2 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:01:00Z', 'max': 4004}]})\n",
      "2 1 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:03:00Z', 'max': 4004}]})\n",
      "2 1 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:04:55Z', 'max': 2004}]})\n",
      "2 2 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:08:20Z', 'max': 2004}]})\n",
      "2 1 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:10:20Z', 'max': 2004}]})\n",
      "2 2 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:12:20Z', 'max': 4004}]})\n",
      "4 2 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:14:20Z', 'max': 8008}]})\n",
      "4 2 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:38:40Z', 'max': 4008}]})\n",
      "4 2 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T09:48:10Z', 'max': 0}]})\n",
      "4 2 ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:52:45Z', 'max': 8008}]})\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import time\n",
    "\n",
    "for param in params:\n",
    "    # get experimentation boundaries\n",
    "    start_time = max(param['_agg_clients']['start_time'], param['_agg_servers']['start_time'])\n",
    "    stop_time = max(param['_agg_clients']['stop_time'], param['_agg_servers']['stop_time'])\n",
    "    duration = stop_time - start_time\n",
    "\n",
    "    tar = os.path.join(RESULT_PATH, param['backup_dir'], 'influxdb-data.tar.gz')\n",
    "    tarfile.open(tar).extractall()\n",
    "    #docker run --name influxdb -v $(pwd)/influxdb-data:/var/lib/influxdb -p 8083:8083 -p 8086:8086 -ti influxdb\n",
    "    try:\n",
    "        container = client.containers.run(\n",
    "            'influxdb',\n",
    "            detach=True,\n",
    "            ports={'8086/tcp': 8086, '8083/tcp': 8083},\n",
    "            volumes={os.path.join(os.getcwd(), 'influxdb-data'): {'bind': '/var/lib/influxdb', 'mode': 'rw'}}\n",
    "        )    \n",
    "        influx = InfluxDBClient(database='telegraf')\n",
    "        # TODO(msimonin): make a tcp socket retry test on port 8083\n",
    "        time.sleep(5)\n",
    "        result = influx.query(\"select max(messages_published) from rabbitmq_overview\")\n",
    "        print(param[\"nbr_clients\"], param[\"nbr_servers\"], result)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        container.remove(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResultSet({'('rabbitmq_overview', None)': [{'time': '2017-12-18T10:10:20Z', 'max': 2004}]})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
